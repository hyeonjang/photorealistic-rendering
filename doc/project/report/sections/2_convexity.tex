\section*{Discussion of Convexity}\label{ch:ch2label}
We discuss the convexity of the objective function by observing that $U(c,l)$ is a weighted sum of functions depending on $c_{t}, l_{t}$.
\\
\begin{equation*}
\implies U(c,l) = \sum_{t=0}^{N-1}{\beta^{t}}[f(c_t,l_t)]
\end{equation*}
Let us consider $\beta^t, c_t, f(c_t, l_t)$ and $l_t$ as vectors of length N. $\rightarrow$ $\beta, c, f(c, l), l$.
\begin{equation*}
\implies U(c,l) = \beta^T[f(c,l)]
\end{equation*}
We see that $U(c,l)$ is an affine function of $f(c,l)$, and that $\beta \succeq 0$. Therefore, for the optimization problem to be convex, we need to analyze the concavity of $f(c,l)$. In this case, because of the affine combination property, it is sufficient to check if $f(c_t,l_t)$ is concave. We look at the Hessian of $f(c_t,l_t)$.\\

\begin{minipage}{0.4\textwidth}
\begin{align*}
\frac{\partial f}{\partial c_t} = \frac{1}{c_t} \implies \frac{\partial^2 f}{\partial c_t^2}= -\frac{1}{c_t^2}
\end{align*}
\end{minipage}
\begin{minipage}{0.6\textwidth}
\begin{align*}
\frac{\partial f}{\partial l_t}= -\frac{1}{1-l_t} \implies \frac{\partial^2 f}{\partial l_t^2}= -\frac{1}{(1-l_t)^2}
\end{align*}
\end{minipage}
\\
\begin{minipage}{0.5\textwidth}
\begin{align*}
\frac{\partial^2 f}{\partial c_t \partial l_t}= 0
\end{align*}
\end{minipage}
\begin{minipage}{0.5\textwidth}
\begin{align*}
\bigtriangledown^2f(c_t,l_t)= \begin{bmatrix}
-\frac{1}{c_t^2} & 0 \\
0 & -\frac{1}{(1-l_t)^2}
\end{bmatrix}
\preceq 0
\end{align*}
\end{minipage}
\\\\
The Hessian is a negative matrix, and therefore $f(c_t,l_t)$ is concave. Since each added term in the sum is concave, we conclude that the objective is a concave function.\\
\\For the constraints, we analyze the market-clearing conditions introduced in the previous section.\\
\begin{minipage}{0.15\textwidth}
\begin{align*}
s_{t+1}=s_{t}=0
\end{align*}
\end{minipage}
\begin{minipage}{0.15\textwidth}
\begin{align*}
c_{t}=l_{t}
\end{align*}
\end{minipage}
\begin{minipage}{0.15\textwidth}
\begin{align*}
P_t c_{t}=m_{t}
\end{align*}
\end{minipage}
\begin{minipage}{0.55\textwidth}
\begin{align*}
\implies m_{t+1}=m_t+\tau_t=m_t+gm_t=m_t(1+g)
\end{align*}
\end{minipage}
\\
\\All constraints are linear and thus convex; therefore we can solve the problem using convex optimization techniques.





